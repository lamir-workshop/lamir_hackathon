<!DOCTYPE html>


<html lang="en" data-content_root="../">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>How do we annotate? &#8212; My sample book</title>



  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only {
        display: none !important;
      }
    </style>
  </noscript>

  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
  <link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

  <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
  <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
  <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
  <link rel="stylesheet" type="text/css"
    href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
  <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
  <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />

  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

  <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
  <script src="../_static/doctools.js?v=9a2dae69"></script>
  <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
  <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
  <script src="../_static/copybutton.js?v=f281be69"></script>
  <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
  <script>let toggleHintShow = 'Click to show';</script>
  <script>let toggleHintHide = 'Click to hide';</script>
  <script>let toggleOpenOnPrint = 'true';</script>
  <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
  <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
  <script src="../_static/design-tabs.js?v=f930bc37"></script>
  <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
  <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
  <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
  <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
  <script>DOCUMENTATION_OPTIONS.pagename = 'ch2_basics/collab';</script>
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en" />
  <meta name="docsearch:version" content="" />
</head>


<body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%"
  data-default-mode="">



  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  <div id="pst-scroll-pixel-helper"></div>

  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>


  <dialog id="pst-search-dialog">

    <form class="bd-search d-flex align-items-center" action="../search.html" method="get">
      <i class="fa-solid fa-magnifying-glass"></i>
      <input type="search" class="form-control" name="q" placeholder="Search this book..."
        aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
      <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
    </form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
    <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
  </div>


  <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
  </header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">





      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">



        <div class="sidebar-header-items sidebar-primary__section">




        </div>

        <div class="sidebar-primary-items__start sidebar-primary__section">
          <div class="sidebar-primary-item">





            <a class="navbar-brand logo" href="../intro.html">










              <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home" />
              <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home" />


            </a>
          </div>
          <div class="sidebar-primary-item">

            <button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search"
              data-bs-placement="bottom" data-bs-toggle="tooltip">
              <i class="fa-solid fa-magnifying-glass"></i>
              <span class="search-button__default-text">Search</span>
              <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd
                  class="kbd-shortcut__modifier">K</kbd></span>
            </button>
          </div>
          <div class="sidebar-primary-item">
            <nav class="bd-links bd-docs-nav" aria-label="Main">
              <div class="bd-toc-item navbar-nav active">

                <ul class="nav bd-sidenav bd-sidenav__home-link">
                  <li class="toctree-l1">
                    <a class="reference internal" href="../intro.html">
                      LAMIR Hackathon 2024
                    </a>
                  </li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intorduction</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch1_intro/tutorial_structure.html">Tutorial structure and setup</a></li>
                  <li class="toctree-l1"><a class="reference internal" href="../ch1_intro/tutorial_scope.html">Tutorial
                      scope and prerequisites</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Setting up a deep learning
                    project</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal" href="wandb.html">Weights and Biases</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Beat tracking with few
                    data</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch3_going_deep/beat_tracking_with_few_data.html">Design decisions for tempo, beat, and
                      downbeat</a></li>




                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Source separation with few
                    data and artificial mixtures</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch4_going_deeper/source_separation_with_few_data.html">Hands on!</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Discussion and
                    conclusions</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch5_discussion/open_challenges.html">Concluding remarks</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch6_resources/references.html">References</a></li>
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch6_resources/acknowledgments.html">Acknowledgments</a></li>
                  <li class="toctree-l1"><a class="reference internal" href="../ch6_resources/authors.html">About the
                      Authors</a></li>
                </ul>

              </div>
            </nav>
          </div>
        </div>


        <div class="sidebar-primary-items__end sidebar-primary__section">
        </div>

        <div id="rtd-footer-container"></div>


      </div>

      <main id="main-content" class="bd-main" role="main">



        <div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">

            <div class="bd-header-article d-print-none">
              <div class="header-article-items header-article__inner">

                <div class="header-article-items__start">

                  <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm"
                      title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
                      <span class="fa-solid fa-bars"></span>
                    </button></div>

                </div>


                <div class="header-article-items__end">

                  <div class="header-article-item">

                    <div class="article-header-buttons">





                      <div class="dropdown dropdown-source-buttons">
                        <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown"
                          aria-expanded="false" aria-label="Source repositories">
                          <i class="fab fa-github"></i>
                        </button>
                        <ul class="dropdown-menu">



                          <li><a href="https://github.com/xavijuanola/lamirtest" target="_blank"
                              class="btn btn-sm btn-source-repository-button dropdown-item" title="Source repository"
                              data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fab fa-github"></i>
                              </span>
                              <span class="btn__text-container">Repository</span>
                            </a>
                          </li>




                          <li><a
                              href="https://github.com/xavijuanola/lamirtest/issues/new?title=Issue%20on%20page%20%2Fch2_basics/collab.html&body=Your%20issue%20content%20here."
                              target="_blank" class="btn btn-sm btn-source-issues-button dropdown-item"
                              title="Open an issue" data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-lightbulb"></i>
                              </span>
                              <span class="btn__text-container">Open issue</span>
                            </a>
                          </li>

                        </ul>
                      </div>






                      <div class="dropdown dropdown-download-buttons">
                        <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown"
                          aria-expanded="false" aria-label="Download this page">
                          <i class="fas fa-download"></i>
                        </button>
                        <ul class="dropdown-menu">



                          <li><a href="../_sources/ch2_basics/collab.md" target="_blank"
                              class="btn btn-sm btn-download-source-button dropdown-item" title="Download source file"
                              data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-file"></i>
                              </span>
                              <span class="btn__text-container">.md</span>
                            </a>
                          </li>




                          <li>
                            <button onclick="window.print()" class="btn btn-sm btn-download-pdf-button dropdown-item"
                              title="Print to PDF" data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-file-pdf"></i>
                              </span>
                              <span class="btn__text-container">.pdf</span>
                            </button>
                          </li>

                        </ul>
                      </div>




                      <button onclick="toggleFullScreen()" class="btn btn-sm btn-fullscreen-button"
                        title="Fullscreen mode" data-bs-placement="bottom" data-bs-toggle="tooltip">


                        <span class="btn__icon-container">
                          <i class="fas fa-expand"></i>
                        </span>

                      </button>



                      <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only"
                        aria-label="Color mode" data-bs-title="Color mode" data-bs-placement="bottom"
                        data-bs-toggle="tooltip">
                        <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light"
                          title="Light"></i>
                        <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark" title="Dark"></i>
                        <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"
                          title="System Settings"></i>
                      </button>


                      <button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only"
                        title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
                        <i class="fa-solid fa-magnifying-glass fa-lg"></i>
                      </button>
                      <button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar"
                        data-bs-placement="bottom" data-bs-toggle="tooltip">
                        <span class="fa-solid fa-list"></span>
                      </button>
                    </div>
                  </div>

                </div>

              </div>
            </div>



            <div id="jb-print-docs-body" class="onlyprint">
              <h1>How do we annotate?</h1>
              <!-- Table of contents -->
              <div id="print-main-content">
                <div id="jb-print-toc">

                  <div>
                    <h2> Contents </h2>
                  </div>
                  <nav aria-label="Page">
                    <ul class="visible nav section-nav flex-column">
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#manual-annotation-example">Manual annotation example</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#perspectives-on-manual-annotation">Perspectives on manual annotation</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#towards-automating-the-annotation-process">Towards automating the annotation
                          process</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#example-annotated-datasets">Example annotated datasets</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#summary">Summary</a></li>
                    </ul>
                  </nav>
                </div>
              </div>
            </div>



            <div id="searchbox"></div>
            <article class="bd-article">

              <section class="tex2jax_ignore mathjax_ignore" id="how-do-we-annotate">
                <span id="annotatemap"></span>
                <h1>How do we annotate?<a class="headerlink" href="#how-do-we-annotate"
                    title="Link to this heading">#</a></h1>
                <p>Within the context of the kind of data-driven approaches for the estimation
                  of tempo, beats, and downbeats we explore in this tutorial, a critical aspect is
                  how to acquire some data to learn from, and ultimately to evaluate upon.</p>
                <p>The “data” in question refers to annotations of beat and downbeat
                  locations from which either: i) a global value tempo for a roughly
                  constant tempo piece of music; or ii) a local tempo contour can be derived.</p>
                <p>The workflow by which beat and downbeat annotations can be obtained
                  typically involves an interative process departing from an initial estimate,
                  e.g., marking beat locations only, and then correcting timing errors, followed by
                  a labelling process to mark the metrical position of each beat.</p>
                <p>This initial estimate could be obtained by hand, i.e., by tapping
                  along with the musical audio excerpt in a software such as <code
                    class="docutils literal notranslate"><span class="pre">Sonic</span> <span class="pre">Visualiser</span></code>,
                  or alternatively, by running an existing beat estimation algorithm, e.g., from <code
                    class="docutils literal notranslate"><span class="pre">madmom</span></code> or <code
                    class="docutils literal notranslate"><span class="pre">librosa</span></code> and then loading this
                  annotation layer.</p>
                <p>We can consider these approaches to be <strong>manual</strong> or <strong>semi-automatic</strong>.
                  Within the beat tracking literature and the creation of datasets,
                  both approaches have been used. To begin with, we’ll focus on the fully manual approach.</p>
                <section id="manual-annotation-example">
                  <span id="annotatemap-example"></span>
                  <h2>Manual annotation example<a class="headerlink" href="#manual-annotation-example"
                      title="Link to this heading">#</a></h2>
                  <p>The figure below gives an illustration of a typical manual annotation process
                    in <code
                      class="docutils literal notranslate"><span class="pre">Sonic</span> <span class="pre">Visualiser</span></code>.
                    The excerpt in quesiton is the straightforward musical excerpt
                    from the previous section that we’ve already here, and is around 25s in duration
                    with a constant tempo and 4/4 metre.</p>
                  <div class="admonition note">
                    <p class="admonition-title">Note</p>
                    <p>The clip is sped up by a factor of 5, so you shouldn’t expect to hear anything!</p>
                  </div>
                  <figure class="align-center" id="annotate">
                    <a class="reference internal image-reference"
                      href="assets/ch2_basics/figs/annotation_process.gif"><img
                        alt="Annotation example in Sonic Visualiser."
                        src="assets/ch2_basics/figs/annotation_process.gif" style="width: 1200px;" /></a>
                    <figcaption>
                      <p><span class="caption-text">Manual annotation example in Sonic Visualiser.</span><a
                          class="headerlink" href="#annotate" title="Link to this image">#</a></p>
                    </figcaption>
                  </figure>
                  <p>The stages of the process as follows:</p>
                  <ol class="arabic simple">
                    <li>
                      <p>Listening to the excerpt and tapping along in real time to mark the beat locations.
                        Note, usually it’s not possible to start tapping straight away as it takes some time
                        for a listener to infer the beat (even for familiar pieces of music).
                        In this case, the tapper begins at the start of the 3rd bar, meaning the first two bars will
                        need to be filled in later.</p>
                    </li>
                    <li>
                      <p>Having completed one real-time pass over the musical excerpt, the next stage is to go
                        back and listen again, but this time with the beat annotations rendered as audible
                        clicks of short duration. As becomes clear from watching the clip above,
                        the timing of the taps is not super precise! As such many of the beats need to be altered to
                        compensate for temporal imprecision.
                        While this could simply be slopping timing on the part of the
                        tapper, in practice it is likely a a combination of human motor noise and jitter <a
                          class="footnote-reference brackets" href="#id11" id="id1" role="doc-noteref"><span
                            class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> in the acquisition of the
                        keyboard taps.
                        In this case, there are no duplicated or missing taps
                        (besides those of the first two bars), and so the tap-editing operations are exclusively
                        performed by shifting the annotations – using the waveform as a guide – and listening back for
                        perceptual accuracy.</p>
                    </li>
                    <li>
                      <p>Once done, the annotations for the first two bars are marked approximately by hand
                        and the main listening and editing process in the previous step is applied again.</p>
                    </li>
                    <li>
                      <p>In this excerpt, there is a constant 4/4 metre throughout, thus it is straightforward
                        to have Sonic Visualiser apply two-level labels to the beat locations, 1.1, 1.2., 1.3,
                        1.4, 2.1, 2.2., etc. where each ‘x.1’ corresponds to a downbeat. However, in more complex cases
                        containing changes in metre, it may be necessary to edit the annotation labels by hand. Having
                        performed this labelling, a final listen and minor edits are made, the process is complete and
                        the annotations can be exported.</p>
                    </li>
                  </ol>
                </section>
                <section id="perspectives-on-manual-annotation">
                  <span id="annotatemap-perspectives"></span>
                  <h2>Perspectives on manual annotation<a class="headerlink" href="#perspectives-on-manual-annotation"
                      title="Link to this heading">#</a></h2>
                  <p>At this point it is worth a little reflection the practical aspects of the manual annotation and
                    editing process. The musical excerpt is under 25s in duration
                    yet the total time taken to complete the annotation is a little over 4 minutes
                    (approximately a <strong>10x</strong> overhead). Concerning the number and type of edits, we find 8
                    insertions: corresponding to the first two bars, 0 deletions, and 21 shifting operations (including
                    some beats shifted more than once) for a total of 32 beat annotations.</p>
                  <p>For brevity, the annotation corrections were made rather quickly with an emphasis on
                    approximate perceptual accuracy as opposed to extremely precise hand-labelling.
                    A more “forensic” analysis of the waveform (perhaps supported by other
                    time-frequency representations) and additional listening back would further increase the
                    annotation time. Of course, the better the real-time taps the fewer repeated listens and
                    editing operations, but in the limit even when no edits are required, this would still
                    requires two complete listens (once to tap, once to confirm).</p>
                  <p>If we then begin to consider more complex musical material, e.g., with challenging
                    musical properties such as syncopation, expressive timing, metrical changes, lack of
                    percussion etc. together the cognitive burden of annotating and annotator fatigue it’s
                    easy to imagine that the annotation process could be 1-2 orders of magnitude more
                    time-consuming. As we’ve seen in the case of the expressive excerpt in the previous
                    section, specific aspects of the annotation may require access to a musical score (if it exists and
                    is available).</p>
                  <div class="admonition note">
                    <p class="admonition-title">Note</p>
                    <p>For comparison, the expressive piece in its entirety (4m51s) which is used extensively in <span
                        id="id2">[<a class="reference internal" href="../ch6_resources/references.html#id7"
                          title="António S Pinto, Sebastian Böck, Jaime S Cardoso, and Matthew EP Davies. User-driven fine-tuning for beat tracking. Electronics, 10(13):1518, 2021.">PBockCD21</a>]</span>
                      took around 15 hours to annotate (spread over 3 days), and included frequent discussion with
                      musical experts.</p>
                  </div>
                  <p><strong>Why does this matter?</strong> This matters in the context of deep learning,
                    since we’d typically like to acquire as much high-quality annotated data as possible
                    when training models. Thus if it is very expensive and time-consuming to accurately annotate then
                    this may intrinsically limit the potential of deep learning approaches.</p>
                  <p>Furthermore, it is also worthwhile to consider the type of musical material will be annotated since
                    it’s not just about “how much” but also “what.” While straightforward
                    musical excerpts like the one shown above are essentially easy to annotate
                    there may be little benefit in annotating this kind of musical content since it
                    is already “trackable.” On this basis, the added benefit in annotation likely
                    resides in more challenging musical material which takes longer to annotate
                    and may be more ambiguous.</p>
                </section>
                <section id="towards-automating-the-annotation-process">
                  <span id="annotatemap-automating"></span>
                  <h2>Towards automating the annotation process<a class="headerlink"
                      href="#towards-automating-the-annotation-process" title="Link to this heading">#</a></h2>
                  <p>Given the labour-intensive nature of the annotation process, it is useful
                    to consider possible steps for at least partial automation.</p>
                  <ul class="simple">
                    <li>
                      <p>Instead of performing real-time tapping to make an initial estimate
                        of the beat, it’s possible to execute an existing beat tracking algorithm.
                        For easier examples this may be highly beneficial as the issues
                        relating to (human) motor noise and jitter can be avoided.
                        However, we must accept that the temporal accuracy of the beat locations will be
                        quantised to the frame rate at which the beats are estimated (e.g., every 10ms),
                        and so may still require some fine temporal adjustment. Perhaps more
                        troubling is that the choice of metrical level will be determined
                        by an algorithm and thus may bias the annotator who may have
                        otherwise chosen to tap at a different metrical level.
                        Finally, if the material is extremely complex, and “beyond the scope”
                        of what existing approaches can reliably annotate, then there
                        may be very little value in an initial automatic first pass
                        if all beat estimates need subsequent correction.</p>
                    </li>
                    <li>
                      <p>A promising approach for the automatic correction of annotations
                        appeared in ISMIR 2019 paper by Drieger et al <span id="id3">[<a class="reference internal"
                            href="../ch6_resources/references.html#id235"
                            title="Jonathan Driedger, Hendrik Schreiber, W. Bas de Haas, and Meinard Müller. Towards automatically correcting tapped beat annotations for music recordings. In Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR). Delft, The Netherlands, November 2019.">DSdHMuller19</a>]</span>
                        which relies on a smart snapping of manually-tapped beat locations
                        to peaks in a beat activation function (i.e., the prediction
                        of a deep neural network) or an onset detection function, and is shown below.<br />
                        While shown to be successful in improving the subjective quality
                        of annotations according to musical experts, the full work-flow
                        in the paper still recommends a final human intervention to confirm
                        the precise annotations. Furthermore the success of the approach
                        depends on the presence of peaks in the beat activation function
                        at, or near, the “correct” locations.</p>
                    </li>
                  </ul>
                  <figure class="align-center" id="tapcorrect">
                    <a class="reference internal image-reference" href="assets/ch2_basics/figs/tapcorrect.png"><img
                        alt="Tap Correction Procedure Overivew." src="assets/ch2_basics/figs/tapcorrect.png"
                        style="width: 400px;" /></a>
                    <figcaption>
                      <p><span class="caption-text">Tap Correction Procedure Overivew. Image taken from <span
                            id="id4">[<a class="reference internal" href="../ch6_resources/references.html#id235"
                              title="Jonathan Driedger, Hendrik Schreiber, W. Bas de Haas, and Meinard Müller. Towards automatically correcting tapped beat annotations for music recordings. In Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR). Delft, The Netherlands, November 2019.">DSdHMuller19</a>]</span></span><a
                          class="headerlink" href="#tapcorrect" title="Link to this image">#</a></p>
                    </figcaption>
                  </figure>
                  <ul class="simple">
                    <li>
                      <p>Finally, in the case of sequenced music, (e.g., electronic dance music) it may possible
                        to be obtain a global tempo, beat, and downbeat locations automatically from
                        the project settings. For example, in the Giant Steps tempo dataset <span id="id5">[<a
                            class="reference internal" href="../ch6_resources/references.html#id236"
                            title="Peter Knees, Angel Faraldo, Perfecto Herrera, Richard Vogl, Sebastian Böck, Florian Hörschläger, and Mickael Le Goff. Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections. In Proc. of the 16th Intl. Society for Music Information Retrieval Conf. (ISMIR), 364–370. 2015.">KFH+15</a>]</span>
                        tempo labels were obtained directly from the online music service Beatport, but
                        but many cases were found to be ambiguous, or incorrect and required relabelling.
                        Of course, it’s worth remembering that not all sequenced music need be of constant tempo and
                        metre, nor are they necessarily easy to analyse (even in cases of constant tempo).</p>
                    </li>
                  </ul>
                </section>
                <section id="example-annotated-datasets">
                  <span id="annotatemap-datasets"></span>
                  <h2>Example annotated datasets<a class="headerlink" href="#example-annotated-datasets"
                      title="Link to this heading">#</a></h2>
                  <p>To provide some additional perspective on annotation, and continue the thread
                    from the previous point about the Giant Steps tempo dataset, we can take a look at a
                    small set of example datasets and highlight some relevant insights concerning
                    the type of musical material they contain, and the manner in which they
                    were annotated.</p>
                  <div class="admonition note">
                    <p class="admonition-title">Note</p>
                    <p>This list is in no way intended to be exhaustive,
                      merely a subset that help support the tutorial content. For a list
                      of MIR datasets of all kinds, please see Alexander Lerch’s Audio Content Analysis <a
                        class="reference external" href="https://www.audiocontentanalysis.org/data-sets/">website</a>.
                    </p>
                    <p>We’ll learn more about <code
                        class="docutils literal notranslate"><span class="pre">mirdata</span></code> in the practical
                      part of the tutorial
                      and the importance of loading and working with the correct version of a dataset.</p>
                  </div>
                  <ul class="simple">
                    <li>
                      <p><strong>Hainsworth</strong> <span id="id6">[<a class="reference internal"
                            href="../ch6_resources/references.html#id161"
                            title="Stephen W Hainsworth and Malcolm D Macleod. Particle filtering applied to musical tempo tracking. EURASIP Journal on Advances in Signal Processing, 2004(15):927847, 2004.">HM04</a>]</span>
                        The <strong>Hainsworth</strong> dataset comprises 222 musical excerpts of around 1 minute
                        each in length, which were organised into six categories by genre label:
                        rock/pop, dance, jazz, folk, classical, and choral. It was created by
                        Stephen Hainsworth as part of his PhD thesis on automatic music transcription.
                        He produced the annotations in a two-stage process, first
                        recording initial taps and then subsequently using a custom interface
                        in Matlab to load and then manually correct the annotations guided
                        by a time-frequency representation. Note, this dataset
                        pre-dates even the earliest versions of Sonic Visualiser by a few years
                        and thus Stephen needed to make his own tool for annotation correction.
                        Of particular note was the inclusion of 20 or so choral examples
                        which drew first attention in the beat tracking community to a particularly
                        challenging class of musical signals to annotate, and analyse.
                        The Hainsworth dataset remained in its original incarnation
                        until 2014 when Böck et al <span id="id7">[<a class="reference internal"
                            href="../ch6_resources/references.html#id241"
                            title="Sebastian Böck, Florian Krebs, and Gerhard Widmer. A multi-model approach to beat tracking considering heterogeneous music styles. In Proc. of the 15th Intl. Society for Music Information Retrieval Conf. (ISMIR), 603–608. Taiwan, Tapei, 2014.">BockKW14b</a>]</span>
                        performed
                        a set of revisions on the beat and downbeat annotations
                        to correct some errors, and resulted in an increase in performance.</p>
                    </li>
                    <li>
                      <p><strong>HJDB</strong> <span id="id8">[<a class="reference internal"
                            href="../ch6_resources/references.html#id188"
                            title="Jason Hockman, Matthew EP Davies, and Ichiro Fujinaga. One in the jungle: downbeat detection in hardcore, jungle, and drum and bass. In ISMIR, 169–174. 2012.">HDF12</a>]</span>
                        The <strong>HJDB</strong> dataset has 236 excerpts taken from 80s and 90s
                        electronic dance music, specifically in the sub-genres of hardcore,
                        jungle, and drum and bass. Initially it was only annotated
                        in terms of downbeat positions (with no beat positions).
                        Subsequently a set of beat annotations, and revisions to the
                        downbeats were made, and the current “reference” set of annotations
                        can be found among the supplementary material for <span id="id9">[<a class="reference internal"
                            href="../ch6_resources/references.html#id5"
                            title="Sebastian Böck, Matthew EP Davies, and Peter Knees. Multi-task learning of tempo and beat: learning one to improve the other. In ISMIR, 486–493. 2019.">BockDK19</a>]</span>
                        which can be found <a class="reference external"
                          href="https://github.com/superbock/ISMIR2019">here</a>.
                        This dataset is somewhat noteworthy as being among the first datasets
                        to push back against the notion that electronic dance music is essentially
                        very straightforward from the perspective of computational
                        rhythm analysis.</p>
                    </li>
                    <li>
                      <p><strong>SMC</strong> <span id="id10">[<a class="reference internal"
                            href="../ch6_resources/references.html#id238"
                            title="André Holzapfel, Matthew E. P. Davies, José R. Zapata, João Lobato Oliveira, and Fabien Gouyon. Selective sampling for beat tracking evaluation. IEEE Transactions on Audio, Speech, and Language Processing, 20(9):2539-2548, 2012. doi:10.1109/TASL.2012.2205244.">HDZ+12</a>]</span>
                        The <strong>SMC</strong> dataset contains 217 excerpts of 40s each in duration and
                        was designed with a methodology for selecting the audio examples to annotate.
                        Specifically, it was based on the idea of building a dataset
                        out of musical audio examples that would be difficult for (then) state-of-the-art
                        beat tracking algorithms to analyse. Normally, we can discover
                        when the state of the art fails by running it on an audio excerpt
                        for which ground truth annotations exist, and then use one or more
                        evaluation methods to estimate performance. Thus, the challenge here
                        was to identify these kinds of excerpts <strong>without</strong> annotating them first.
                        The approach was to build a committee of “good, but different” beat
                        tracking algorithms and run them over a large collection of unannotated
                        musical audio signals. Next, a subset of excerpts was selected based
                        on the lack of concensus among the estimates of each committee member.
                        In effect, if all algorithms gave a different answer, then
                        this would at least hint at interesting properties in the music
                        that would make it worthwhile to annotate.
                        Despite this dataset being compiled around 10 years ago (and thus predating
                        almost all work in deep learning applied to rhythm), it remains
                        highly challenging even for the most recent state-of-the-art
                        approaches of the kind we’ll explore later in the tutorial.</p>
                    </li>
                  </ul>
                </section>
                <section id="summary">
                  <span id="annotatemap-summary"></span>
                  <h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
                  <ul class="simple">
                    <li>
                      <p>Annotation is hard!</p>
                    </li>
                    <li>
                      <p>It takes a long time, and the more challenging
                        the material to annotate the greater the likelihood of this being
                        helpful for learning.</p>
                    </li>
                    <li>
                      <p>On the plus side, annotation is a fantastic
                        way to learn about the task of beat and downbeat estimation
                        so it’s a really great excercise.</p>
                    </li>
                    <li>
                      <p>We always need more data,
                        so do consider doing some annotating!</p>
                    </li>
                    <li>
                      <p>As hard as we try, annotation “mistakes” are made, so they made need correcting.</p>
                    </li>
                    <li>
                      <p>This makes comparative evaluation more challenging, so it’s always worthwile
                        to ensure you are using the most up to date version of any annotations.</p>
                    </li>
                  </ul>
                  <hr class="footnotes docutils" />
                  <aside class="footnote-list brackets">
                    <aside class="footnote brackets" id="id11" role="doc-footnote">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span
                          class="fn-bracket">]</span></span>
                      <p>Recent work by Anglada-Tort et al <span id="id12">[<a class="reference internal"
                            href="../ch6_resources/references.html#id237"
                            title="Manuel Anglada-Tort, Peter MC Harrison, and Nori Jacoby. Repp: a robust cross-platform solution for online sensorimotor synchronization experiments. bioRxiv, 2021.">ATHJ21</a>]</span>
                        has propsed a means to eliminate jitter through a novel signal acquisition approach.</p>
                    </aside>
                  </aside>
                </section>
              </section>

              <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ch2_basics"
        },
        predefinedOutput: true
    }
    </script>
              <script>kernelName = 'python3'</script>

            </article>






            <footer class="prev-next-footer d-print-none">

              <div class="prev-next-area">
              </div>
            </footer>

          </div>



          <dialog id="pst-secondary-sidebar-modal"></dialog>
          <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
            <div class="sidebar-secondary-items sidebar-secondary__inner">


              <div class="sidebar-secondary-item">
                <div class="page-toc tocsection onthispage">
                  <i class="fa-solid fa-list"></i> Contents
                </div>
                <nav class="bd-toc-nav page-toc">
                  <ul class="visible nav section-nav flex-column">
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#manual-annotation-example">Manual annotation example</a></li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#perspectives-on-manual-annotation">Perspectives on manual annotation</a></li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#towards-automating-the-annotation-process">Towards automating the annotation process</a>
                    </li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#example-annotated-datasets">Example annotated datasets</a></li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#summary">Summary</a></li>
                  </ul>
                </nav>
              </div>

            </div>
          </div>


        </div>
        <footer class="bd-footer-content">

          <div class="bd-footer-content__inner container">

            <div class="footer-item">

              <p class="component-author">
                By Matthew E. P. Davies, Sebastian Bock, Magdalena Fuentes
              </p>

            </div>

            <div class="footer-item">


              <p class="copyright">

                © Copyright 2024.
                <br />

              </p>

            </div>

            <div class="footer-item">

            </div>

            <div class="footer-item">

            </div>

          </div>
        </footer>


      </main>
    </div>
  </div>

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
  <script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
</body>

</html>
<!DOCTYPE html>


<html lang="en" data-content_root="../">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Deep learning approaches &#8212; My sample book</title>



  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only {
        display: none !important;
      }
    </style>
  </noscript>

  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
  <link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

  <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
  <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
  <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
  <link rel="stylesheet" type="text/css"
    href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
  <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
  <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />

  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

  <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
  <script src="../_static/doctools.js?v=9a2dae69"></script>
  <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
  <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
  <script src="../_static/copybutton.js?v=f281be69"></script>
  <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
  <script>let toggleHintShow = 'Click to show';</script>
  <script>let toggleHintHide = 'Click to hide';</script>
  <script>let toggleOpenOnPrint = 'true';</script>
  <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
  <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
  <script src="../_static/design-tabs.js?v=f930bc37"></script>
  <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
  <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
  <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
  <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
  <script>DOCUMENTATION_OPTIONS.pagename = 'ch3_going_deep/overview';</script>
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en" />
  <meta name="docsearch:version" content="" />
</head>


<body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%"
  data-default-mode="">



  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>

  <div id="pst-scroll-pixel-helper"></div>

  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>


  <dialog id="pst-search-dialog">

    <form class="bd-search d-flex align-items-center" action="../search.html" method="get">
      <i class="fa-solid fa-magnifying-glass"></i>
      <input type="search" class="form-control" name="q" placeholder="Search this book..."
        aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
      <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
    </form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
    <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
  </div>


  <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
  </header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">





      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">



        <div class="sidebar-header-items sidebar-primary__section">




        </div>

        <div class="sidebar-primary-items__start sidebar-primary__section">
          <div class="sidebar-primary-item">





            <a class="navbar-brand logo" href="../intro.html">










              <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home" />
              <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home" />


            </a>
          </div>
          <div class="sidebar-primary-item">

            <button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search"
              data-bs-placement="bottom" data-bs-toggle="tooltip">
              <i class="fa-solid fa-magnifying-glass"></i>
              <span class="search-button__default-text">Search</span>
              <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd
                  class="kbd-shortcut__modifier">K</kbd></span>
            </button>
          </div>
          <div class="sidebar-primary-item">
            <nav class="bd-links bd-docs-nav" aria-label="Main">
              <div class="bd-toc-item navbar-nav active">

                <ul class="nav bd-sidenav bd-sidenav__home-link">
                  <li class="toctree-l1">
                    <a class="reference internal" href="../intro.html">
                      LAMIR Hackathon 2024
                    </a>
                  </li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intorduction</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch1_intro/tutorial_structure.html">Tutorial structure and setup</a></li>
                  <li class="toctree-l1"><a class="reference internal" href="../ch1_intro/tutorial_scope.html">Tutorial
                      scope and prerequisites</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Setting up a deep learning
                    project</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal" href="../ch2_basics/wandb.html">Weights and
                      Biases</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Beat tracking with few
                    data</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal" href="beat_tracking_with_few_data.html">Design
                      decisions for tempo, beat, and downbeat</a></li>




                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Source separation with few
                    data and artificial mixtures</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch4_going_deeper/source_separation_with_few_data.html">Hands on!</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Discussion and
                    conclusions</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch5_discussion/open_challenges.html">Concluding remarks</a></li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch6_resources/references.html">References</a></li>
                  <li class="toctree-l1"><a class="reference internal"
                      href="../ch6_resources/acknowledgments.html">Acknowledgments</a></li>
                  <li class="toctree-l1"><a class="reference internal" href="../ch6_resources/authors.html">About the
                      Authors</a></li>
                </ul>

              </div>
            </nav>
          </div>
        </div>


        <div class="sidebar-primary-items__end sidebar-primary__section">
        </div>

        <div id="rtd-footer-container"></div>


      </div>

      <main id="main-content" class="bd-main" role="main">



        <div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">

            <div class="bd-header-article d-print-none">
              <div class="header-article-items header-article__inner">

                <div class="header-article-items__start">

                  <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm"
                      title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
                      <span class="fa-solid fa-bars"></span>
                    </button></div>

                </div>


                <div class="header-article-items__end">

                  <div class="header-article-item">

                    <div class="article-header-buttons">





                      <div class="dropdown dropdown-source-buttons">
                        <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown"
                          aria-expanded="false" aria-label="Source repositories">
                          <i class="fab fa-github"></i>
                        </button>
                        <ul class="dropdown-menu">



                          <li><a href="https://github.com/xavijuanola/lamirtest" target="_blank"
                              class="btn btn-sm btn-source-repository-button dropdown-item" title="Source repository"
                              data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fab fa-github"></i>
                              </span>
                              <span class="btn__text-container">Repository</span>
                            </a>
                          </li>




                          <li><a
                              href="https://github.com/xavijuanola/lamirtest/issues/new?title=Issue%20on%20page%20%2Fch3_going_deep/overview.html&body=Your%20issue%20content%20here."
                              target="_blank" class="btn btn-sm btn-source-issues-button dropdown-item"
                              title="Open an issue" data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-lightbulb"></i>
                              </span>
                              <span class="btn__text-container">Open issue</span>
                            </a>
                          </li>

                        </ul>
                      </div>






                      <div class="dropdown dropdown-download-buttons">
                        <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown"
                          aria-expanded="false" aria-label="Download this page">
                          <i class="fas fa-download"></i>
                        </button>
                        <ul class="dropdown-menu">



                          <li><a href="../_sources/ch3_going_deep/overview.md" target="_blank"
                              class="btn btn-sm btn-download-source-button dropdown-item" title="Download source file"
                              data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-file"></i>
                              </span>
                              <span class="btn__text-container">.md</span>
                            </a>
                          </li>




                          <li>
                            <button onclick="window.print()" class="btn btn-sm btn-download-pdf-button dropdown-item"
                              title="Print to PDF" data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-file-pdf"></i>
                              </span>
                              <span class="btn__text-container">.pdf</span>
                            </button>
                          </li>

                        </ul>
                      </div>




                      <button onclick="toggleFullScreen()" class="btn btn-sm btn-fullscreen-button"
                        title="Fullscreen mode" data-bs-placement="bottom" data-bs-toggle="tooltip">


                        <span class="btn__icon-container">
                          <i class="fas fa-expand"></i>
                        </span>

                      </button>



                      <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only"
                        aria-label="Color mode" data-bs-title="Color mode" data-bs-placement="bottom"
                        data-bs-toggle="tooltip">
                        <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light"
                          title="Light"></i>
                        <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark" title="Dark"></i>
                        <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"
                          title="System Settings"></i>
                      </button>


                      <button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only"
                        title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
                        <i class="fa-solid fa-magnifying-glass fa-lg"></i>
                      </button>
                      <button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar"
                        data-bs-placement="bottom" data-bs-toggle="tooltip">
                        <span class="fa-solid fa-list"></span>
                      </button>
                    </div>
                  </div>

                </div>

              </div>
            </div>



            <div id="jb-print-docs-body" class="onlyprint">
              <h1>Deep learning approaches</h1>
              <!-- Table of contents -->
              <div id="print-main-content">
                <div id="jb-print-toc">

                  <div>
                    <h2> Contents </h2>
                  </div>
                  <nav aria-label="Page">
                    <ul class="visible nav section-nav flex-column">
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#feature-extraction">Feature extraction</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#likelihood-estimation">Likelihood estimation</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#inference">Inference</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next">Next</a>
                      </li>
                    </ul>
                  </nav>
                </div>
              </div>
            </div>



            <div id="searchbox"></div>
            <article class="bd-article">

              <section class="tex2jax_ignore mathjax_ignore" id="deep-learning-approaches">
                <span id="dnns-overview"></span>
                <h1>Deep learning approaches<a class="headerlink" href="#deep-learning-approaches"
                    title="Link to this heading">#</a></h1>
                <p>The goal of these sections is to provide the theoretical underpinnings of deep learning models or
                  <em>deep neural networks</em> (DNNs) commonly used for tempo, beat and downbeat tracking, to build an
                  understanding of why these models work well in these tasks and what are their limitations. Here we
                  discuss the main concepts and definitions,
                  but for those interested in reading more about the use of DNNs in audio processing and MIR related
                  tasks, we suggest them to take a look at works like McFee <span id="id1">[<a
                      class="reference internal" href="../ch6_resources/references.html#id226"
                      title="Brian McFee. Statistical Methods for Scene and Event Classification, pages 103–146. Springer International Publishing, Cham, 2018. URL: https://doi.org/10.1007/978-3-319-63450-0_5, doi:10.1007/978-3-319-63450-0_5.">McF18</a>]</span>,
                  Purwins et al. <span id="id2">[<a class="reference internal"
                      href="../ch6_resources/references.html#id228"
                      title="Hendrik Purwins, Bo Li, Tuomas Virtanen, Jan Schlüter, Shuo-Yiin Chang, and Tara Sainath. Deep learning for audio signal processing. IEEE Journal of Selected Topics in Signal Processing, 13(2):206–219, 2019.">PLV+19</a>]</span>
                  and Choi et al. <span id="id3">[<a class="reference internal"
                      href="../ch6_resources/references.html#id222"
                      title="Keunwoo Choi, György Fazekas, Kyunghyun Cho, and Mark Sandler. A tutorial on deep learning for music information retrieval. arXiv preprint arXiv:1709.04396, 2017.">CFCS17</a>]</span>.
                </p>
                <p>The field moved quite fast these past years and the amount different design choices and approaches
                  can be somehow overwhelming!
                  We tried to come up with a way of summarizing the different moving parts of beat and downbeat deep
                  learning systems to help
                  digest this, as we explain below.</p>
                <p>Recent beat and downbeat tracking approaches can be structured in three main stages: 1) A first stage
                  of <em>low-level feature</em> computation or <em>feature extraction</em>, where feature vectors that
                  represent the content of musical audio are
                  extracted from the raw audio signal (e.g. Spectrogram, Chromagram); 2) A second step that usually
                  consists of a stage of feature learning, whose outcome is an activation function that indicates the
                  most likely candidates
                  for beats and/or downbeats among the input audio observations; 3) Finally, a post-processing stage is
                  often used, usually consisting of a probabilistic graphical model which encodes some relevant musical
                  rules
                  to select the final beat/downbeat candidates.</p>
                <figure class="align-default" id="id25">
                  <img alt="General pipeline commonly used for beat and/or downbeat tracking systems."
                    src="assets/ch3_going_deep/figs/diagram.png" />
                  <figcaption>
                    <p><span class="caption-text">General pipeline commonly used for beat and/or downbeat tracking
                        systems.</span><a class="headerlink" href="#id25" title="Link to this image">#</a></p>
                  </figcaption>
                </figure>
                <p>Different alternatives were proposed for the distinct stages among beat and downbeat tracking
                  systems. Here we give an overview of the main ideas
                  presented in the literature.</p>
                <section id="feature-extraction">
                  <h2>Feature extraction<a class="headerlink" href="#feature-extraction"
                      title="Link to this heading">#</a></h2>
                  <p>It is common to exploit music knowledge for feature design using signal processing techniques.
                    The three most explored categories of musically inspired features in the literature for both beat
                    and downbeat estimation are: chroma (CH) <span id="id4">[<a class="reference internal"
                        href="../ch6_resources/references.html#id31"
                        title="M. Fuentes, B. McFee, H.C. Crayencour, S. Essid, and J.P. Bello. A music structure informed downbeat tracking system using skip-chain conditional random fields and deep learning. In 44th Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 481-485. Brighton, UK, May 2019.">FMC+19</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id9"
                        title="Magdalena Fuentes, Brian McFee, Hélène Crayencour, Slim Essid, and Juan Bello. Analysis of common design choices in deep learning systems for downbeat tracking. In The 19th International Society for Music Information Retrieval Conference. 2018.">FMC+18</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id188"
                        title="Jason Hockman, Matthew EP Davies, and Ichiro Fujinaga. One in the jungle: downbeat detection in hardcore, jungle, and drum and bass. In ISMIR, 169–174. 2012.">HDF12</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id189"
                        title="Maksim Khadkevich, Thomas Fillon, Gaël Richard, and Maurizio Omologo. A probabilistic approach to simultaneous extraction of beats and downbeats. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 445–448. IEEE, 2012.">KFRO12</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id12"
                        title="F. Krebs, S. Böck, M. Dorfer, and G. Widmer. Downbeat tracking using beat synchronous features with recurrent neural networks. In 17th International Society for Music Information Retrieval Conference (ISMIR). 2016.">KBockDW16</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id177"
                        title="Hélene Papadopoulos and Geoffroy Peeters. Joint estimation of chords and downbeats from an audio signal. IEEE Transactions on Audio, Speech, and Language Processing, 19(1):138–152, 2010.">PP10a</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id180"
                        title="Geoffroy Peeters and Helene Papadopoulos. Simultaneous beat and downbeat-tracking using a probabilistic framework: theory and large-scale evaluation. IEEE Transactions on Audio, Speech, and Language Processing, 19(6):1754–1769, 2010.">PP10b</a>]</span>
                    —used to reflect the harmonic content of the signal—,
                    onset detection function (ODF) <span id="id5">[<a class="reference internal"
                        href="../ch6_resources/references.html#id11"
                        title="S. Durand, J. P. Bello, B. David, and G. Richard. Downbeat tracking with multiple features and deep neural networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume. 2015. doi:10.1109/ICASSP.2015.7178001.">DBDR15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id188"
                        title="Jason Hockman, Matthew EP Davies, and Ichiro Fujinaga. One in the jungle: downbeat detection in hardcore, jungle, and drum and bass. In ISMIR, 169–174. 2012.">HDF12</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id203"
                        title="José R Zapata, Matthew EP Davies, and Emilia Gómez. Multi-feature beat tracking. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):816–825, 2014.">ZDGomez14</a>]</span>
                    or spectral flux (SF) <span id="id6">[<a class="reference internal"
                        href="../ch6_resources/references.html#id31"
                        title="M. Fuentes, B. McFee, H.C. Crayencour, S. Essid, and J.P. Bello. A music structure informed downbeat tracking system using skip-chain conditional random fields and deep learning. In 44th Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 481-485. Brighton, UK, May 2019.">FMC+19</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id9"
                        title="Magdalena Fuentes, Brian McFee, Hélène Crayencour, Slim Essid, and Juan Bello. Analysis of common design choices in deep learning systems for downbeat tracking. In The 19th International Society for Music Information Retrieval Conference. 2018.">FMC+18</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id33"
                        title="A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Tracking the 'odd': meter inference in a culturally diverse music corpus. In 15th Int. Society for Music Information Retrieval Conf. (ISMIR), 425-430. Taipei, Taiwan, October 2014.">HKS14</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id189"
                        title="Maksim Khadkevich, Thomas Fillon, Gaël Richard, and Maurizio Omologo. A probabilistic approach to simultaneous extraction of beats and downbeats. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 445–448. IEEE, 2012.">KFRO12</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id191"
                        title="Florian Krebs, Sebastian Böck, and Gerhard Widmer. Rhythmic pattern modeling for beat and downbeat tracking in musical audio. In ISMIR, 227–232. 2013.">KBockW13</a>]</span>
                    —as event-oriented indicators— and timbre inspired features <span id="id7">[<a
                        class="reference internal" href="../ch6_resources/references.html#id201"
                        title="Simon Durand, Bertrand David, and Gaël Richard. Enhancing downbeat detection when facing different music styles. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3132–3136. IEEE, 2014.">DDR14</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id188"
                        title="Jason Hockman, Matthew EP Davies, and Ichiro Fujinaga. One in the jungle: downbeat detection in hardcore, jungle, and drum and bass. In ISMIR, 169–174. 2012.">HDF12</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id195"
                        title="Ajay Srinivasamurthy, André Holzapfel, and Xavier Serra. In search of automatic rhythm analysis methods for turkish and indian art music. Journal of New Music Research, 43(1):94–114, 2014.">SHS14</a>]</span>
                    such as spectral coefficients or MFCCs.
                    For beat, the main features exploited are those related to event-oriented indicators, assuming that
                    changes in the spectral energy relate to
                    beat positions are <span id="id8">[<a class="reference internal"
                        href="../ch6_resources/references.html#id181"
                        title="Norberto Degara, Enrique Argones Rúa, Antonio Pena, Soledad Torres-Guijarro, Matthew EP Davies, and Mark D Plumbley. Reliability-informed beat tracking of musical signals. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):290–301, 2012.">DRuaP+12</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id29"
                        title="T. Fillon, C. Joder, S. Durand, and S. Essid. A conditional random field system for beat tracking. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 424-428. South Brisbane, Australia, April 2015.">FJDE15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id33"
                        title="A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Tracking the 'odd': meter inference in a culturally diverse music corpus. In 15th Int. Society for Music Information Retrieval Conf. (ISMIR), 425-430. Taipei, Taiwan, October 2014.">HKS14</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id191"
                        title="Florian Krebs, Sebastian Böck, and Gerhard Widmer. Rhythmic pattern modeling for beat and downbeat tracking in musical audio. In ISMIR, 227–232. 2013.">KBockW13</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id204"
                        title="Florian Krebs, Andre Holzapfel, Ali Taylan Cemgil, and Gerhard Widmer. Inferring metrical structure in music using particle filters. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(5):817–827, 2015.">KHCW15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id105"
                        title="L. Nunes, M. Rocamora, L. Jure, and L. W. P. Biscainho. Beat and downbeat tracking based on rhythmic patterns applied to the uruguayan candombe drumming. In 16th Int. Soc. for Music Information Retrieval Conf. (ISMIR), 264-270. Málaga, Spain, October 2015.">NRJB15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id52"
                        title="A. Srinivasamurthy, A. Holzapfel, A. T. Cemgil, and X. Serra. Particle filters for efficient meter tracking with dynamic bayesian networks. In 16th Int. Society for Music Information Retrieval Conf. (ISMIR). 2015. URL: http://hdl.handle.net/10230/34998.">SHCS15</a>]</span>.
                    For downbeat, harmonic-related features showed to be relevant to estimate downbeats reliably across
                    music genres.</p>
                  <p>The feature extraction is usually based on a single feature <span id="id9">[<a
                        class="reference internal" href="../ch6_resources/references.html#id63"
                        title="S. Böck, F. Krebs, and G. Widmer. A Multi-model Approach to Beat Tracking Considering Heterogeneous Music Styles. In 15th Conf. of the Int. Soc. for Music Information Retrieval (ISMIR 2014), 603-608. Taipei, Taiwan, October 2014.">BockKW14a</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id33"
                        title="A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Tracking the 'odd': meter inference in a culturally diverse music corpus. In 15th Int. Society for Music Information Retrieval Conf. (ISMIR), 425-430. Taipei, Taiwan, October 2014.">HKS14</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id202"
                        title="Filip Korzeniowski, Sebastian Böck, and Gerhard Widmer. Probabilistic extraction of beat positions from a beat activation function. In ISMIR, 513–518. 2014.">KBockW14</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id19"
                        title="F. Krebs, S. Böck, and G. Widmer. An efficient state space model for joint tempo and meter tracking. In 16th International Society for Music Information Retrieval Conference (ISMIR). 2011.">KBockW11</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id177"
                        title="Hélene Papadopoulos and Geoffroy Peeters. Joint estimation of chords and downbeats from an audio signal. IEEE Transactions on Audio, Speech, and Language Processing, 19(1):138–152, 2010.">PP10a</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id180"
                        title="Geoffroy Peeters and Helene Papadopoulos. Simultaneous beat and downbeat-tracking using a probabilistic framework: theory and large-scale evaluation. IEEE Transactions on Audio, Speech, and Language Processing, 19(6):1754–1769, 2010.">PP10b</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id52"
                        title="A. Srinivasamurthy, A. Holzapfel, A. T. Cemgil, and X. Serra. Particle filters for efficient meter tracking with dynamic bayesian networks. In 16th Int. Society for Music Information Retrieval Conf. (ISMIR). 2015. URL: http://hdl.handle.net/10230/34998.">SHCS15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id203"
                        title="José R Zapata, Matthew EP Davies, and Emilia Gómez. Multi-feature beat tracking. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):816–825, 2014.">ZDGomez14</a>]</span>,
                    with some exceptions exploiting more than one music property at the same time <span id="id10">[<a
                        class="reference internal" href="../ch6_resources/references.html#id11"
                        title="S. Durand, J. P. Bello, B. David, and G. Richard. Downbeat tracking with multiple features and deep neural networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume. 2015. doi:10.1109/ICASSP.2015.7178001.">DBDR15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id60"
                        title="S. Durand, J. P. Bello, B. David, and G. Richard. Feature adapted convolutional neural networks for downbeat tracking. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP). 2016.">DBDR16</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id65"
                        title="S. Durand, J. P. Bello, B. David, and G. Richard. Robust Downbeat Tracking Using an Ensemble of Convolutional Networks. IEEE/ACM Trans. on Audio, Speech, and Language Processing, 25(1):76-89, January 2017.">DBDR17</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id31"
                        title="M. Fuentes, B. McFee, H.C. Crayencour, S. Essid, and J.P. Bello. A music structure informed downbeat tracking system using skip-chain conditional random fields and deep learning. In 44th Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 481-485. Brighton, UK, May 2019.">FMC+19</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id9"
                        title="Magdalena Fuentes, Brian McFee, Hélène Crayencour, Slim Essid, and Juan Bello. Analysis of common design choices in deep learning systems for downbeat tracking. In The 19th International Society for Music Information Retrieval Conference. 2018.">FMC+18</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id12"
                        title="F. Krebs, S. Böck, M. Dorfer, and G. Widmer. Downbeat tracking using beat synchronous features with recurrent neural networks. In 17th International Society for Music Information Retrieval Conference (ISMIR). 2016.">KBockDW16</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id203"
                        title="José R Zapata, Matthew EP Davies, and Emilia Gómez. Multi-feature beat tracking. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):816–825, 2014.">ZDGomez14</a>]</span>,
                    which results in systems robust to different music genres <span id="id11">[<a
                        class="reference internal" href="../ch6_resources/references.html#id65"
                        title="S. Durand, J. P. Bello, B. David, and G. Richard. Robust Downbeat Tracking Using an Ensemble of Convolutional Networks. IEEE/ACM Trans. on Audio, Speech, and Language Processing, 25(1):76-89, January 2017.">DBDR17</a>]</span>.
                    Recently, approaches based on deep learning exploring combinations of logarithmic spectrograms with
                    different resolutions showed to perform competently <span id="id12">[<a class="reference internal"
                        href="../ch6_resources/references.html#id6"
                        title="Sebastian Böck and Matthew EP Davies. Deconstruct, analyse, reconstruct: how to improve tempo, beat, and downbeat estimation. Proc. of ISMIR (International Society for Music Information Retrieval). Montreal, Canada, pages 574–582, 2020.">BockD20</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id5"
                        title="Sebastian Böck, Matthew EP Davies, and Peter Knees. Multi-task learning of tempo and beat: learning one to improve the other. In ISMIR, 486–493. 2019.">BockDK19</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id214"
                        title="S. Böck, F. Krebs, and G. Widmer. Joint beat and downbeat tracking with recurrent neural networks. In 17th International Society for Music Information Retrieval Conference (ISMIR). 2016.">BockKW16</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id202"
                        title="Filip Korzeniowski, Sebastian Böck, and Gerhard Widmer. Probabilistic extraction of beat positions from a beat activation function. In ISMIR, 513–518. 2014.">KBockW14</a>]</span>.
                  </p>
                  <figure class="align-default" id="id26">
                    <img alt="Example of features used for downbeat tracking."
                      src="assets/ch3_going_deep/figs/features_example.png" />
                    <figcaption>
                      <p><span class="caption-text">Example of features, from left to right: melodic constant-Q
                          transform, onset detection function, chromagram, low-frequency spectrogram. Adapted from <span
                            id="id13">[<a class="reference internal" href="../ch6_resources/references.html#id65"
                              title="S. Durand, J. P. Bello, B. David, and G. Richard. Robust Downbeat Tracking Using an Ensemble of Convolutional Networks. IEEE/ACM Trans. on Audio, Speech, and Language Processing, 25(1):76-89, January 2017.">DBDR17</a>]</span>.</span><a
                          class="headerlink" href="#id26" title="Link to this image">#</a></p>
                    </figcaption>
                  </figure>
                </section>
                <section id="likelihood-estimation">
                  <h2>Likelihood estimation<a class="headerlink" href="#likelihood-estimation"
                      title="Link to this heading">#</a></h2>
                  <p>The objective of this stage is to map the input representation into a beat/downbeat likelihood that
                    indicates which are the most likely candidates to be a beat or a downbeat
                    in a given temporal sequence. There are two main groups of approaches in this respect: the first one
                    uses “heuristics” to perform the mapping, while the second
                    group exploits machine learning approaches. The latter group is the most popular one in the
                    literature in the last years and also the state of the art.</p>
                  <p>The estimation of a likelihood with heuristics is performed differently depending on the features
                    used. For instance, a common approach is to pre-define a template of <em>expected</em> features such
                    as
                    spectral-flux or chroma, and to measure the distance between this template to the features computed
                    from the audio signal <span id="id14">[<a class="reference internal"
                        href="../ch6_resources/references.html#id105"
                        title="L. Nunes, M. Rocamora, L. Jure, and L. W. P. Biscainho. Beat and downbeat tracking based on rhythmic patterns applied to the uruguayan candombe drumming. In 16th Int. Soc. for Music Information Retrieval Conf. (ISMIR), 264-270. Málaga, Spain, October 2015.">NRJB15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id180"
                        title="Geoffroy Peeters and Helene Papadopoulos. Simultaneous beat and downbeat-tracking using a probabilistic framework: theory and large-scale evaluation. IEEE Transactions on Audio, Speech, and Language Processing, 19(6):1754–1769, 2010.">PP10b</a>]</span>.
                    Within the group of
                    machine learning approaches, we could identify two subgroups: a first one that exploits
                    “traditional” learning techniques and a second one with focus on deep learning models.</p>
                  <figure class="align-default" id="id27">
                    <img alt="Example of rhythmic patter learning."
                      src="assets/ch3_going_deep/figs/rhythmic_patterns.png" />
                    <figcaption>
                      <p><span class="caption-text">Example of rhythmic patter learning from <span id="id15">[<a
                              class="reference internal" href="../ch6_resources/references.html#id191"
                              title="Florian Krebs, Sebastian Böck, and Gerhard Widmer. Rhythmic pattern modeling for beat and downbeat tracking in musical audio. In ISMIR, 227–232. 2013.">KBockW13</a>]</span>.</span><a
                          class="headerlink" href="#id27" title="Link to this image">#</a></p>
                    </figcaption>
                  </figure>
                  <p>Before deep learning, machine learning systems often focus on recognizing rhythm patterns in data,
                    for instance by using <em>Gaussian Mixture Models</em> (GMM) and k-means
                    <span id="id16">[<a class="reference internal" href="../ch6_resources/references.html#id33"
                        title="A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Tracking the 'odd': meter inference in a culturally diverse music corpus. In 15th Int. Society for Music Information Retrieval Conf. (ISMIR), 425-430. Taipei, Taiwan, October 2014.">HKS14</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id191"
                        title="Florian Krebs, Sebastian Böck, and Gerhard Widmer. Rhythmic pattern modeling for beat and downbeat tracking in musical audio. In ISMIR, 227–232. 2013.">KBockW13</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id204"
                        title="Florian Krebs, Andre Holzapfel, Ali Taylan Cemgil, and Gerhard Widmer. Inferring metrical structure in music using particle filters. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(5):817–827, 2015.">KHCW15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id105"
                        title="L. Nunes, M. Rocamora, L. Jure, and L. W. P. Biscainho. Beat and downbeat tracking based on rhythmic patterns applied to the uruguayan candombe drumming. In 16th Int. Soc. for Music Information Retrieval Conf. (ISMIR), 264-270. Málaga, Spain, October 2015.">NRJB15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id52"
                        title="A. Srinivasamurthy, A. Holzapfel, A. T. Cemgil, and X. Serra. Particle filters for efficient meter tracking with dynamic bayesian networks. In 16th Int. Society for Music Information Retrieval Conf. (ISMIR). 2015. URL: http://hdl.handle.net/10230/34998.">SHCS15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id47"
                        title="A. Srinivasamurthy, A. Holzapfel, A. T. Cemgil, and X. Serra. A generalized bayesian model for tracking long metrical cycles in acoustic music signals. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 76-80. Shanghai, China, March 2016.">SHCS16</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id225"
                        title="Ajay Srinivasamurthy, Andre Holzapfel, and Xavier Serra. Informed automatic meter analysis of music recordings. In ISMIR-International Conference on Music Information Retrieval. 2017.">SHS17</a>]</span>.
                    This usually required making some assumptions of
                    style or genre (e.g. to define the length of the patterns to be learned), and for these models to be
                    effective the music should have distinctive rhythmic patterns.
                    Deep learning approaches propose an alternative to such limitations given their capacity to learn
                    complex function mappings, and systems exploiting DNNs have became the state of the art in
                    recent years <span id="id17">[<a class="reference internal"
                        href="../ch6_resources/references.html#id231"
                        title="Bijue Jia, Jiancheng Lv, and Dayiheng Liu. Deep learning-based automatic downbeat tracking: a brief review. Multimedia Systems, pages 1–22, 2019.">JLL19</a>]</span>.
                  </p>
                  <figure class="align-default" id="id28">
                    <img alt="Example of likelihood estimation."
                      src="assets/ch3_going_deep/figs/feature_extraction.png" />
                    <figcaption>
                      <p><span class="caption-text">Different stages of feature extraction. Left: input spectrogram,
                          middle: intermediate DNN outputs, right: the final beat and downbeat likelihoods. Adapted from
                          <span id="id18">[<a class="reference internal" href="../ch6_resources/references.html#id214"
                              title="S. Böck, F. Krebs, and G. Widmer. Joint beat and downbeat tracking with recurrent neural networks. In 17th International Society for Music Information Retrieval Conference (ISMIR). 2016.">BockKW16</a>]</span>.</span><a
                          class="headerlink" href="#id28" title="Link to this image">#</a></p>
                    </figcaption>
                  </figure>
                </section>
                <section id="inference">
                  <h2>Inference<a class="headerlink" href="#inference" title="Link to this heading">#</a></h2>
                  <p>The aim of this stage is to obtain the final downbeat sequence by selecting the most likely
                    candidates in the downbeat likelihood given some model or criteria. Probabilistic graphical models
                    (PGMs) are the most used
                    post-processing techniques since 2010. This might be due to two main reasons: PGMs offer a flexible
                    framework to incorporate music knowledge and then exploit interrelated structure <span id="id19">[<a
                        class="reference internal" href="../ch6_resources/references.html#id177"
                        title="Hélene Papadopoulos and Geoffroy Peeters. Joint estimation of chords and downbeats from an audio signal. IEEE Transactions on Audio, Speech, and Language Processing, 19(1):138–152, 2010.">PP10a</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id180"
                        title="Geoffroy Peeters and Helene Papadopoulos. Simultaneous beat and downbeat-tracking using a probabilistic framework: theory and large-scale evaluation. IEEE Transactions on Audio, Speech, and Language Processing, 19(6):1754–1769, 2010.">PP10b</a>]</span>,
                    and the
                    Bar Pointer Model (BPM) <span id="id20">[<a class="reference internal"
                        href="../ch6_resources/references.html#id55"
                        title="N. Whiteley, A. T. Cemgil, and S. J. Godsill. Bayesian modelling of temporal structure in musical audio. In 7th Int. Society for Music Information Retrieval Conf. (ISMIR). Citeseer, 2006.">WCG06</a>]</span>
                    stands as a very effective and adaptable model for meter tracking, being popular for beat and
                    downbeat tracking.</p>
                  <p>PGMs proved to be adaptable to cultural-aware systems in diverse music cultures <span id="id21">[<a
                        class="reference internal" href="../ch6_resources/references.html#id33"
                        title="A. Holzapfel, F. Krebs, and A. Srinivasamurthy. Tracking the 'odd': meter inference in a culturally diverse music corpus. In 15th Int. Society for Music Information Retrieval Conf. (ISMIR), 425-430. Taipei, Taiwan, October 2014.">HKS14</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id105"
                        title="L. Nunes, M. Rocamora, L. Jure, and L. W. P. Biscainho. Beat and downbeat tracking based on rhythmic patterns applied to the uruguayan candombe drumming. In 16th Int. Soc. for Music Information Retrieval Conf. (ISMIR), 264-270. Málaga, Spain, October 2015.">NRJB15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id195"
                        title="Ajay Srinivasamurthy, André Holzapfel, and Xavier Serra. In search of automatic rhythm analysis methods for turkish and indian art music. Journal of New Music Research, 43(1):94–114, 2014.">SHS14</a>]</span>,
                    being for instance extendable to track longer meter cycles and different meters than the widely
                    explored 3/4 and 4/4 <span id="id22">[<a class="reference internal"
                        href="../ch6_resources/references.html#id47"
                        title="A. Srinivasamurthy, A. Holzapfel, A. T. Cemgil, and X. Serra. A generalized bayesian model for tracking long metrical cycles in acoustic music signals. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 76-80. Shanghai, China, March 2016.">SHCS16</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id225"
                        title="Ajay Srinivasamurthy, Andre Holzapfel, and Xavier Serra. Informed automatic meter analysis of music recordings. In ISMIR-International Conference on Music Information Retrieval. 2017.">SHS17</a>]</span>.
                    Considerable efforts have been made towards improving the use of these models in practice, by
                    reducing computational cost via an efficient state-space definition <span id="id23">[<a
                        class="reference internal" href="../ch6_resources/references.html#id19"
                        title="F. Krebs, S. Böck, and G. Widmer. An efficient state space model for joint tempo and meter tracking. In 16th International Society for Music Information Retrieval Conference (ISMIR). 2011.">KBockW11</a>]</span>
                    or proposing <em>sequential Monte Carlo</em> methods (also called <em>particle filters</em>) for
                    inference <span id="id24">[<a class="reference internal"
                        href="../ch6_resources/references.html#id204"
                        title="Florian Krebs, Andre Holzapfel, Ali Taylan Cemgil, and Gerhard Widmer. Inferring metrical structure in music using particle filters. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(5):817–827, 2015.">KHCW15</a>,
                      <a class="reference internal" href="../ch6_resources/references.html#id52"
                        title="A. Srinivasamurthy, A. Holzapfel, A. T. Cemgil, and X. Serra. Particle filters for efficient meter tracking with dynamic bayesian networks. In 16th Int. Society for Music Information Retrieval Conf. (ISMIR). 2015. URL: http://hdl.handle.net/10230/34998.">SHCS15</a>]</span>.
                  </p>
                </section>
                <section id="next">
                  <h2>Next<a class="headerlink" href="#next" title="Link to this heading">#</a></h2>
                  <p>After discussing the usual pipelines of beat and downbeat tracking systems, let’s dig a bit more in
                    the different deep learning architectures used for likelihood estimation these past years.</p>
                </section>
              </section>

              <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ch3_going_deep"
        },
        predefinedOutput: true
    }
    </script>
              <script>kernelName = 'python3'</script>

            </article>






            <footer class="prev-next-footer d-print-none">

              <div class="prev-next-area">
              </div>
            </footer>

          </div>



          <dialog id="pst-secondary-sidebar-modal"></dialog>
          <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc">
            <div class="sidebar-secondary-items sidebar-secondary__inner">


              <div class="sidebar-secondary-item">
                <div class="page-toc tocsection onthispage">
                  <i class="fa-solid fa-list"></i> Contents
                </div>
                <nav class="bd-toc-nav page-toc">
                  <ul class="visible nav section-nav flex-column">
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#feature-extraction">Feature extraction</a></li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#likelihood-estimation">Likelihood estimation</a></li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#inference">Inference</a></li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next">Next</a>
                    </li>
                  </ul>
                </nav>
              </div>

            </div>
          </div>


        </div>
        <footer class="bd-footer-content">

          <div class="bd-footer-content__inner container">

            <div class="footer-item">

              <p class="component-author">
                By Matthew E. P. Davies, Sebastian Bock, Magdalena Fuentes
              </p>

            </div>

            <div class="footer-item">


              <p class="copyright">

                © Copyright 2024.
                <br />

              </p>

            </div>

            <div class="footer-item">

            </div>

            <div class="footer-item">

            </div>

          </div>
        </footer>


      </main>
    </div>
  </div>

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
  <script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
</body>

</html>